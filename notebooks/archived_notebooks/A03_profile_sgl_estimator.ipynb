{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import afqinsight as afqi\n",
    "import copt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richford/miniconda3/envs/afq-insight/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "afq_data = afqi.load_afq_data('../data/raw/als_data', target_cols=['class'], binary_positives=['ALS'])\n",
    "\n",
    "x, y, groups, columns, bias_index = (\n",
    "    afq_data.x,\n",
    "    afq_data.y['class'],\n",
    "    afq_data.groups,\n",
    "    afq_data.columns,\n",
    "    afq_data.bias_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile stats pickled to file 'lprof_dump'. \n",
      "\n",
      "*** Profile printout saved to text file 'lprof_text0'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 7.12026 s\n",
       "File: /Users/richford/miniconda3/envs/afq-insight/lib/python3.6/site-packages/copt/proxgrad.py\n",
       "Function: minimize_PGD at line 7\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     7                                           def minimize_PGD(\n",
       "     8                                                   f_grad, x0, prox=None, tol=1e-6, max_iter=500, verbose=0,\n",
       "     9                                                   callback=None, backtracking=True, step_size=None,\n",
       "    10                                                   max_iter_backtracking=1000, backtracking_factor=0.6,\n",
       "    11                                                   ):\n",
       "    12                                               \"\"\"Proximal gradient descent.\n",
       "    13                                           \n",
       "    14                                               Solves problems of the form\n",
       "    15                                           \n",
       "    16                                                       minimize_x f(x) + g(x)\n",
       "    17                                           \n",
       "    18                                               where we have access to the gradient of f and to the proximal operator of g.\n",
       "    19                                           \n",
       "    20                                               Parameters\n",
       "    21                                               ----------\n",
       "    22                                               f_grad: callable\n",
       "    23                                                    Returns the function value and gradient of the objective function.\n",
       "    24                                           \n",
       "    25                                               g : penalty term (proximal)\n",
       "    26                                           \n",
       "    27                                               x0 : array-like, optional\n",
       "    28                                                   Initial guess\n",
       "    29                                           \n",
       "    30                                               backtracking : boolean\n",
       "    31                                                   Whether to perform backtracking (i.e. line-search) or not.\n",
       "    32                                           \n",
       "    33                                               max_iter : int\n",
       "    34                                                   Maximum number of iterations.\n",
       "    35                                           \n",
       "    36                                               verbose : int\n",
       "    37                                                   Verbosity level, from 0 (no output) to 2 (output on each iteration)\n",
       "    38                                           \n",
       "    39                                               step_size : float\n",
       "    40                                                   Starting value for the line-search procedure. XXX\n",
       "    41                                           \n",
       "    42                                               callback : callable\n",
       "    43                                                   callback function (optional).\n",
       "    44                                           \n",
       "    45                                               Returns\n",
       "    46                                               -------\n",
       "    47                                               res : The optimization result represented as a\n",
       "    48                                                   ``scipy.optimize.OptimizeResult`` object. Important attributes are:\n",
       "    49                                                   ``x`` the solution array, ``success`` a Boolean flag indicating if\n",
       "    50                                                   the optimizer exited successfully and ``message`` which describes\n",
       "    51                                                   the cause of the termination. See `scipy.optimize.OptimizeResult`\n",
       "    52                                                   for a description of other attributes.\n",
       "    53                                           \n",
       "    54                                               References\n",
       "    55                                               ----------\n",
       "    56                                               Beck, Amir, and Marc Teboulle. \"Gradient-based algorithms with applications to signal\n",
       "    57                                               recovery.\" Convex optimization in signal processing and communications (2009)\n",
       "    58                                               \"\"\"\n",
       "    59         1          1.0      1.0      0.0      x = x0\n",
       "    60         1          1.0      1.0      0.0      if not max_iter_backtracking > 0:\n",
       "    61                                                   raise ValueError('Line search iterations need to be greater than 0')\n",
       "    62                                           \n",
       "    63         1          1.0      1.0      0.0      if prox is None:\n",
       "    64                                                   prox = lambda x, y: x\n",
       "    65                                           \n",
       "    66         1          1.0      1.0      0.0      if step_size is None:\n",
       "    67                                                   step_size = 1\n",
       "    68                                           \n",
       "    69         1          1.0      1.0      0.0      success = False\n",
       "    70         1          1.0      1.0      0.0      certificate = np.NaN\n",
       "    71                                           \n",
       "    72         1          1.0      1.0      0.0      it = 1\n",
       "    73                                               # .. a while loop instead of a for loop ..\n",
       "    74                                               # .. allows for infinite or floating point max_iter ..\n",
       "    75                                           \n",
       "    76         1       5380.0   5380.0      0.1      fk, grad_fk = f_grad(x)\n",
       "    77         1        129.0    129.0      0.0      pbar = trange(max_iter, disable=(verbose == 0))\n",
       "    78      1335       4787.0      3.6      0.1      for it in pbar:\n",
       "    79      1335       1431.0      1.1      0.0          if callback is not None:\n",
       "    80                                                       callback(x)\n",
       "    81                                                   # .. compute gradient and step size\n",
       "    82                                                   # TODO: could compute loss and grad in the same function call\n",
       "    83      1335     647442.0    485.0      9.1          x_next = prox(x - step_size * grad_fk, step_size)\n",
       "    84      1335      15841.0     11.9      0.2          incr = x_next - x\n",
       "    85      1335       1528.0      1.1      0.0          if backtracking:\n",
       "    86      1335       2244.0      1.7      0.0              step_size *= 1.1\n",
       "    87      1573       3897.0      2.5      0.1              for _ in range(max_iter_backtracking):\n",
       "    88      1573    6102905.0   3879.8     85.7                  f_next, grad_next = f_grad(x_next)\n",
       "    89      1573      37674.0     24.0      0.5                  if f_next <= fk + grad_fk.dot(incr) + incr.dot(incr) / (2.0 * step_size):\n",
       "    90                                                               # .. step size found ..\n",
       "    91      1335       1762.0      1.3      0.0                      break\n",
       "    92                                                           else:\n",
       "    93                                                               # .. backtracking, reduce step size ..\n",
       "    94       238        371.0      1.6      0.0                      step_size *= backtracking_factor\n",
       "    95       238     113762.0    478.0      1.6                      x_next = prox(x - step_size * grad_fk, step_size)\n",
       "    96       238       3043.0     12.8      0.0                      incr = x_next - x\n",
       "    97                                                       else:\n",
       "    98                                                           warnings.warn(\"Maxium number of line-search iterations reached\")\n",
       "    99                                                   else:\n",
       "   100                                                       f_next, grad_next = f_grad(x_next)\n",
       "   101      1335      70183.0     52.6      1.0          certificate = np.linalg.norm((x - x_next) / step_size)\n",
       "   102      1335      10655.0      8.0      0.1          x[:] = x_next\n",
       "   103      1335       1544.0      1.2      0.0          fk = f_next\n",
       "   104      1335       2916.0      2.2      0.0          grad_fk = grad_next\n",
       "   105                                           \n",
       "   106      1335       8088.0      6.1      0.1          pbar.set_description('PGD')\n",
       "   107      1335       1464.0      1.1      0.0          if backtracking:\n",
       "   108      1335      80644.0     60.4      1.1              pbar.set_postfix(tol=certificate, step_size=step_size, iter=it)\n",
       "   109                                                   else:\n",
       "   110                                                       pbar.set_postfix(tol=certificate, iter=it)\n",
       "   111                                           \n",
       "   112      1335       2545.0      1.9      0.0          if certificate < tol:\n",
       "   113         1          1.0      1.0      0.0              if verbose:\n",
       "   114                                                           pbar.write(\"Achieved relative tolerance at iteration %s\" % it)\n",
       "   115         1          1.0      1.0      0.0              success = True\n",
       "   116         1          3.0      3.0      0.0              break\n",
       "   117                                               else:\n",
       "   118                                                   warnings.warn(\n",
       "   119                                                       \"minimize_PGD did not reach the desired tolerance level\",\n",
       "   120                                                       RuntimeWarning)\n",
       "   121         1          4.0      4.0      0.0      pbar.close()\n",
       "   122         1          1.0      1.0      0.0      return optimize.OptimizeResult(\n",
       "   123         1          1.0      1.0      0.0          x=x, success=success,\n",
       "   124         1          1.0      1.0      0.0          certificate=certificate,\n",
       "   125         1          2.0      2.0      0.0          nit=it, step_size=step_size)\n",
       "\n",
       "Total time: 6.07087 s\n",
       "File: /Users/richford/miniconda3/envs/afq-insight/lib/python3.6/site-packages/copt/utils.py\n",
       "Function: f_grad at line 132\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   132                                               def f_grad(self, x, return_gradient=True):\n",
       "   133      1574       1770.0      1.1      0.0          if self.intercept:\n",
       "   134                                                       x_, c = x[:-1], x[-1]\n",
       "   135                                                   else:\n",
       "   136      1574       1178.0      0.7      0.0              x_, c = x, 0.\n",
       "   137      1574     163829.0    104.1      2.7          z = safe_sparse_dot(self.A, x_, dense_output=True).ravel() + c\n",
       "   138      1574       7235.0      4.6      0.1          idx = z > 0\n",
       "   139      1574      20242.0     12.9      0.3          loss_vec = np.zeros_like(z)\n",
       "   140      1574    1881338.0   1195.3     31.0          loss_vec[idx] = np.log(1 + np.exp(-z[idx])) + (1 - self.b[idx]) * z[idx]\n",
       "   141      1574    1389732.0    882.9     22.9          loss_vec[~idx] = np.log(1 + np.exp(z[~idx])) - self.b[~idx] * z[~idx]\n",
       "   142      1574      76988.0     48.9      1.3          loss = loss_vec.mean() + .5 * self.alpha * safe_sparse_dot(x_.T, x_, dense_output=True).ravel()[0]\n",
       "   143                                           \n",
       "   144      1574       1481.0      0.9      0.0          if not return_gradient:\n",
       "   145                                                       return loss\n",
       "   146      1574      16603.0     10.5      0.3          z0 = np.zeros_like(z)\n",
       "   147      1574       8911.0      5.7      0.1          tmp = np.exp(-z[idx])\n",
       "   148      1574    1102497.0    700.4     18.2          z0[idx] = - tmp / (1 + tmp) + 1 - self.b[idx]\n",
       "   149      1574       9818.0      6.2      0.2          tmp = np.exp(z[~idx])\n",
       "   150      1574    1093780.0    694.9     18.0          z0[~idx] = tmp / (1 + tmp) - self.b[~idx]\n",
       "   151      1574     242975.0    154.4      4.0          grad = self.A.T.dot(z0) / self.A.shape[0] + self.alpha * x_.T\n",
       "   152      1574       6975.0      4.4      0.1          grad = np.asarray(grad).ravel()\n",
       "   153      1574      42857.0     27.2      0.7          grad_c = z0.mean()\n",
       "   154      1574       1589.0      1.0      0.0          if self.intercept:\n",
       "   155                                                       return np.concatenate((grad, [grad_c]))\n",
       "   156                                           \n",
       "   157      1574       1070.0      0.7      0.0          return loss, grad\n",
       "\n",
       "Total time: 7.18105 s\n",
       "File: /Users/richford/projects/neuro/afq/insight-paper/src/afqinsight/afqinsight/insight.py\n",
       "Function: sgl_estimator at line 120\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   120                                           @registered\n",
       "   121                                           def sgl_estimator(x_train, y_train, x_test, y_test, groups, bias_index=None,\n",
       "   122                                                             beta0=None, alpha1=0.0, alpha2=0.0, max_iter=5000,\n",
       "   123                                                             tol=1e-6, verbose=0, suppress_warnings=True, cb_trace=False,\n",
       "   124                                                             accelerate=False, loss_type='logloss', clf_threshold=0.5):\n",
       "   125                                               \"\"\"Find solution to sparse group lasso problem by proximal gradient descent\n",
       "   126                                           \n",
       "   127                                               Solve sparse group lasso [1]_ problem for feature matrix `x_train` and\n",
       "   128                                               target vector `y_train` with features partitioned into groups. Solve using\n",
       "   129                                               the proximal gradient descent (PGD) algorithm. Compute accuracy and ROC AUC\n",
       "   130                                               using `x_test` and `y_test`.\n",
       "   131                                           \n",
       "   132                                               Parameters\n",
       "   133                                               ----------\n",
       "   134                                               x_train : numpy.ndarray\n",
       "   135                                                   Training feature matrix\n",
       "   136                                           \n",
       "   137                                               y_train : numpy.ndarray\n",
       "   138                                                   Training target array\n",
       "   139                                           \n",
       "   140                                               x_test : numpy.ndarray\n",
       "   141                                                   Testing feature matrix\n",
       "   142                                           \n",
       "   143                                               y_test : numpy.ndarray\n",
       "   144                                                   Testing target array\n",
       "   145                                           \n",
       "   146                                               groups : numpy.ndarray\n",
       "   147                                                   Array of non-overlapping indices for each group. For example, if nine\n",
       "   148                                                   features are grouped into equal contiguous groups of three, then groups\n",
       "   149                                                   would be an nd.array like [[0, 1, 2], [3, 4, 5], [6, 7, 8]].\n",
       "   150                                           \n",
       "   151                                               bias_index : int or None, default=None\n",
       "   152                                                   the index of the bias feature in x_train and x_test. If None, assume\n",
       "   153                                                   no bias feature.\n",
       "   154                                           \n",
       "   155                                               beta0 : numpy.ndarray\n",
       "   156                                                   Initial guess for coefficient array\n",
       "   157                                           \n",
       "   158                                               alpha1 : float, default=0.0\n",
       "   159                                                   Group lasso regularization parameter. This encourages groupwise\n",
       "   160                                                   sparsity.\n",
       "   161                                           \n",
       "   162                                               alpha2 : float, default=0.0\n",
       "   163                                                   Lasso regularization parameter. This encourages within group sparsity.\n",
       "   164                                           \n",
       "   165                                               max_iter : int, default=5000\n",
       "   166                                                   Maximum number of iterations for PGD algorithm.\n",
       "   167                                           \n",
       "   168                                               tol : float, default=1e-6\n",
       "   169                                                   Convergence tolerance for PGD algorithm.\n",
       "   170                                           \n",
       "   171                                               verbose : int, default=0\n",
       "   172                                                   Verbosity flag for PGD algorithm.\n",
       "   173                                           \n",
       "   174                                               suppress_warnings : bool, default=True\n",
       "   175                                                   If True, suppress convergence warnings from PGD algorithm.\n",
       "   176                                                   This is useful for hyperparameter tuning when some combinations\n",
       "   177                                                   of hyperparameters may not converge.\n",
       "   178                                           \n",
       "   179                                               cb_trace : bool, default=False\n",
       "   180                                                   If True, include copt.utils.Trace() object in return\n",
       "   181                                           \n",
       "   182                                               accelerate : bool, default=False\n",
       "   183                                                   If True, use accelerated PGD algorithm, otherwise use standard PGD.\n",
       "   184                                           \n",
       "   185                                               loss_type : {'logloss', 'square', 'huber'}\n",
       "   186                                                   The type of loss function to use. If 'logloss', treat this problem as\n",
       "   187                                                   a binary classification problem using logistic regression. Otherwise,\n",
       "   188                                                   treat this problem as a regression problem using either the mean\n",
       "   189                                                   square error or the Huber loss.\n",
       "   190                                           \n",
       "   191                                               clf_threshold : float, default=0.5\n",
       "   192                                                   Decision threshold for binary classification\n",
       "   193                                           \n",
       "   194                                               Returns\n",
       "   195                                               -------\n",
       "   196                                               collections.namedtuple\n",
       "   197                                                   namedtuple with fields:\n",
       "   198                                                   alpha1 - group lasso regularization parameter,\n",
       "   199                                                   alpha2 - lasso regularization parameter,\n",
       "   200                                                   beta_hat - estimate of the optimal beta,\n",
       "   201                                                   test - scores namedtuple for test set,\n",
       "   202                                                   train - scores namedtuple for train set,\n",
       "   203                                                   trace - copt.utils.Trace object if cv_trace is True, None otherwise\n",
       "   204                                           \n",
       "   205                                               References\n",
       "   206                                               ----------\n",
       "   207                                               .. [1]  Noah Simon, Jerome Friedman, Trevor Hastie & Robert Tibshirani,\n",
       "   208                                                   \"A Sparse-Group Lasso,\" Journal of Computational and Graphical\n",
       "   209                                                   Statistics, vol. 22:2, pp. 231-245, 2012\n",
       "   210                                                   DOI: 10.1080/10618600.2012.681250\n",
       "   211                                               \"\"\"\n",
       "   212         1          4.0      4.0      0.0      n_features = x_train.shape[1]\n",
       "   213                                           \n",
       "   214         1          1.0      1.0      0.0      if beta0 is None:\n",
       "   215         1        121.0    121.0      0.0          beta0 = np.zeros(n_features)\n",
       "   216                                           \n",
       "   217         1         10.0     10.0      0.0      sg1 = SparseGroupL1(alpha1, alpha2, groups, bias_index=bias_index)\n",
       "   218                                           \n",
       "   219         1          2.0      2.0      0.0      if loss_type not in ['logloss', 'square', 'huber']:\n",
       "   220                                                   raise ValueError(\"loss_type must be one of \"\n",
       "   221                                                                    \"['logloss', 'square', 'huber'].\")\n",
       "   222                                           \n",
       "   223         1         34.0     34.0      0.0      ind = np.ones(x_train.shape[1], bool)\n",
       "   224         1          2.0      2.0      0.0      if bias_index is not None:\n",
       "   225         1          3.0      3.0      0.0          ind[bias_index] = False\n",
       "   226                                           \n",
       "   227         1      19295.0  19295.0      0.3      step_size = 1. / cp.utils.get_lipschitz(x_train[:, ind], loss_type)\n",
       "   228         1          3.0      3.0      0.0      if loss_type == 'logloss':\n",
       "   229         1        473.0    473.0      0.0          f_grad = cp.utils.LogLoss(x_train, y_train).f_grad\n",
       "   230                                               elif loss_type == 'huber':\n",
       "   231                                                   f_grad = cp.utils.HuberLoss(x_train, y_train).f_grad\n",
       "   232                                               else:\n",
       "   233                                                   f_grad = cp.utils.SquareLoss(x_train, y_train).f_grad\n",
       "   234                                           \n",
       "   235         1          2.0      2.0      0.0      if cb_trace:\n",
       "   236                                                   cb_tos = cp.utils.Trace()\n",
       "   237                                                   cb_tos(beta0)\n",
       "   238                                               else:\n",
       "   239         1          1.0      1.0      0.0          cb_tos = None\n",
       "   240                                           \n",
       "   241         1          0.0      0.0      0.0      if accelerate:\n",
       "   242                                                   minimizer = cp.minimize_APGD\n",
       "   243                                               else:\n",
       "   244         1          1.0      1.0      0.0          minimizer = cp.minimize_PGD\n",
       "   245                                           \n",
       "   246         1          1.0      1.0      0.0      if suppress_warnings:\n",
       "   247         1          6.0      6.0      0.0          ctx_mgr = warnings.catch_warnings()\n",
       "   248                                               else:\n",
       "   249                                                   ctx_mgr = contextlib.suppress()\n",
       "   250                                           \n",
       "   251         1          8.0      8.0      0.0      with ctx_mgr:\n",
       "   252                                                   # For some metaparameters, minimize_PGD or minimize_APGD might not\n",
       "   253                                                   # reach the desired tolerance level. This might be okay during\n",
       "   254                                                   # hyperparameter optimization. So ignore the warning if the user\n",
       "   255                                                   # specifies suppress_warnings=True\n",
       "   256         1          1.0      1.0      0.0          if suppress_warnings:\n",
       "   257         1         28.0     28.0      0.0              warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
       "   258         1          1.0      1.0      0.0          pgd = minimizer(\n",
       "   259         1          2.0      2.0      0.0              f_grad, beta0, sg1.prox, step_size=step_size,\n",
       "   260         1          1.0      1.0      0.0              max_iter=max_iter, tol=tol, verbose=verbose,\n",
       "   261         1    7154720.0 7154720.0     99.6              callback=cb_tos)\n",
       "   262                                           \n",
       "   263         1         15.0     15.0      0.0      beta_hat = np.copy(pgd.x)\n",
       "   264                                           \n",
       "   265         1          1.0      1.0      0.0      if loss_type == 'logloss':\n",
       "   266         1          1.0      1.0      0.0          train = classification_scores(x=x_train, y=y_train, beta_hat=beta_hat,\n",
       "   267         1       3190.0   3190.0      0.0                                        clf_threshold=clf_threshold)\n",
       "   268         1          1.0      1.0      0.0          test = classification_scores(x=x_test, y=y_test, beta_hat=beta_hat,\n",
       "   269         1       3116.0   3116.0      0.0                                       clf_threshold=clf_threshold)\n",
       "   270                                               else:\n",
       "   271                                                   train = regression_scores(x=x_train, y=y_train, beta_hat=beta_hat)\n",
       "   272                                                   test = regression_scores(x=x_test, y=y_test, beta_hat=beta_hat)\n",
       "   273                                           \n",
       "   274         1          2.0      2.0      0.0      return SGLResult(\n",
       "   275         1          1.0      1.0      0.0          alpha1=alpha1, alpha2=alpha2, beta_hat=beta_hat,\n",
       "   276         1          3.0      3.0      0.0          test=test, train=train, trace=cb_tos\n",
       "   277                                               )\n",
       "\n",
       "Total time: 0.699244 s\n",
       "File: /Users/richford/projects/neuro/afq/insight-paper/src/afqinsight/afqinsight/prox.py\n",
       "Function: prox at line 75\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    75                                               def prox(self, x, step_size):\n",
       "    76                                                   \"\"\"Return the proximal operator of the sparse group lasso penalty\n",
       "    77                                           \n",
       "    78                                                   For the sparse group lasso, we can decompose the penalty into\n",
       "    79                                           \n",
       "    80                                                   .. math::\n",
       "    81                                                       P(\\beta) = P_1(\\beta) + P_2(\\beta)\n",
       "    82                                           \n",
       "    83                                                   where :math:`P_2 = \\alpha_2 || \\beta ||_1` is the lasso penalty and\n",
       "    84                                                   :math:`P_1 = \\alpha_1 \\displaystyle \\sum_{g \\in G} || \\beta_g ||_2`\n",
       "    85                                                   is the group lasso penalty.\n",
       "    86                                           \n",
       "    87                                                   Then the proximal operator is given by\n",
       "    88                                           \n",
       "    89                                                   .. math::\n",
       "    90                                                       \\text{prox}_{\\lambda P_1 + \\lambda P_2} (u) = \\left(\n",
       "    91                                                       \\text{prox}_{\\lambda P_2} \\circ \\text{prox}_{\\lambda P_1}\n",
       "    92                                                       \\right)(u)\n",
       "    93                                           \n",
       "    94                                                   Parameters\n",
       "    95                                                   ----------\n",
       "    96                                                   x : np.ndarray\n",
       "    97                                                       Argument for proximal operator.\n",
       "    98                                           \n",
       "    99                                                   step_size : float\n",
       "   100                                                       Step size for proximal operator\n",
       "   101                                           \n",
       "   102                                                   Returns\n",
       "   103                                                   -------\n",
       "   104                                                   np.ndarray\n",
       "   105                                                       proximal operator of sparse group lasso penalty evaluated on\n",
       "   106                                                       input `x` with step size `step_size`\n",
       "   107                                                   \"\"\"  # noqa: W605\n",
       "   108      1573      59691.0     37.9      8.5          l1_prox = (np.fmax(x - self.alpha_2 * step_size, 0)\n",
       "   109      1573      84649.0     53.8     12.1                     - np.fmax(- x - self.alpha_2 * step_size, 0))\n",
       "   110      1573       9845.0      6.3      1.4          out = l1_prox.copy()\n",
       "   111                                           \n",
       "   112      1573       1565.0      1.0      0.2          if self.bias_index is not None:\n",
       "   113      1573       2438.0      1.5      0.3              out[self.bias_index] = x[self.bias_index]\n",
       "   114                                           \n",
       "   115      1573     218148.0    138.7     31.2          norms = np.linalg.norm(l1_prox[self.groups], axis=1)\n",
       "   116                                           \n",
       "   117      1573       8373.0      5.3      1.2          norm_mask = norms > self.alpha_1 * step_size\n",
       "   118      1573       4577.0      2.9      0.7          mask_idx_true = np.where(norm_mask)[0]\n",
       "   119      1573       5100.0      3.2      0.7          mask_idx_false = np.where(np.logical_not(norm_mask))[0]\n",
       "   120      1573     104566.0     66.5     15.0          groups_true = np.array(self.groups)[mask_idx_true]\n",
       "   121      1573      99139.0     63.0     14.2          groups_false = np.array(self.groups)[mask_idx_false]\n",
       "   122                                           \n",
       "   123      1573       8504.0      5.4      1.2          out[groups_true] -= (step_size * self.alpha_1\n",
       "   124                                                                        * out[groups_true]\n",
       "   125      1573      55312.0     35.2      7.9                               / norms[mask_idx_true, np.newaxis])\n",
       "   126      1573      36181.0     23.0      5.2          out[groups_false] = 0.0\n",
       "   127                                           \n",
       "   128      1573       1156.0      0.7      0.2          return out"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -T lprof_text0 -D lprof_dump -f afqi.sgl_estimator -f copt.utils.LogLoss.f_grad -f copt.minimize_PGD -f afqi.prox.SparseGroupL1.prox afqi.sgl_estimator(x_train=X_train, y_train=y_train, x_test=X_test, y_test=y_test, groups=groups, bias_index=bias_index, alpha1=0.01, alpha2=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg1 = afqi.prox.SparseGroupL1(0.1, 0.1, groups=groups, bias_index=bias_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_features = X_train.shape[1]\n",
    "beta0 = np.zeros(n_features)\n",
    "    \n",
    "fk, grad_fk = copt.utils.LogLoss(X_train, y_train).f_grad(beta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f afqi.prox.SparseGroupL1.prox sg1.prox(beta0 - 1 * grad_fk, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_beta0 = np.random.rand(16001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_beta0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_beta0[groups[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.linalg.norm(test_beta0[groups], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_loop = np.random.rand(len(beta0))\n",
    "out_vec = np.copy(out_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "for i, g in enumerate(groups):\n",
    "    if norms[i] > norms.mean():\n",
    "        out_loop[g] -= 0.1 * out_loop[g] / norms[i]\n",
    "    else:\n",
    "        out_loop[g] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "norm_mask_bool = norms > norms.mean()\n",
    "norm_mask_idx = np.where(norm_mask_bool)[0]\n",
    "group_mask = np.array(groups)[norm_mask_idx]\n",
    "not_group_mask = np.array(groups)[np.where(np.logical_not(norm_mask_bool))]\n",
    "out_vec[group_mask] -= 0.1 * out_vec[group_mask] / norms[norm_mask_idx, np.newaxis]\n",
    "out_vec[not_group_mask] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(groups)[np.where(np.logical_not(norm_mask_bool))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_group_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(out_loop, out_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = np.ones((3, 5), dtype=np.float64)\n",
    "denom = np.array([1, 2, 3], dtype=np.float64)\n",
    "print(num.shape)\n",
    "print(denom.shape)\n",
    "num / denom[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'arg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5e9b2e726d2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmongoexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMongoTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'arg'"
     ]
    }
   ],
   "source": [
    "t = hp.Trials()\n",
    "tm = hp.mongoexp.MongoTrials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hyperopt.base.Trials"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:afq-insight]",
   "language": "python",
   "name": "conda-env-afq-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
